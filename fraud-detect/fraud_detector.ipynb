{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suspended-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT DEPENDENCIES ##\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use(style = 'seaborn')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dated-cheat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set shape is: (590540, 435)\n",
      "test_set shape is: (506691, 434)\n",
      "Wall time: 43.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## DATA IMPORT ## \n",
    "## For this project the data from https://www.kaggle.com/c/ieee-fraud-detection/data has been used. ##\n",
    "\n",
    "data_path = \"./data/\"\n",
    "\n",
    "train = pd.read_csv(data_path + \"train_processed.csv\")\n",
    "test = pd.read_csv(data_path + \"test_processed.csv\")\n",
    "\n",
    "print('train_set shape is: {}'.format(train.shape))\n",
    "print('test_set shape is: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "digital-valve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>dist2</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>D12</th>\n",
       "      <th>D13</th>\n",
       "      <th>D14</th>\n",
       "      <th>D15</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>M8</th>\n",
       "      <th>M9</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>V36</th>\n",
       "      <th>V37</th>\n",
       "      <th>V38</th>\n",
       "      <th>V39</th>\n",
       "      <th>V40</th>\n",
       "      <th>V41</th>\n",
       "      <th>V42</th>\n",
       "      <th>V43</th>\n",
       "      <th>V44</th>\n",
       "      <th>V45</th>\n",
       "      <th>V46</th>\n",
       "      <th>V47</th>\n",
       "      <th>V48</th>\n",
       "      <th>V49</th>\n",
       "      <th>V50</th>\n",
       "      <th>V51</th>\n",
       "      <th>V52</th>\n",
       "      <th>V53</th>\n",
       "      <th>V54</th>\n",
       "      <th>V55</th>\n",
       "      <th>V56</th>\n",
       "      <th>V57</th>\n",
       "      <th>V58</th>\n",
       "      <th>V59</th>\n",
       "      <th>V60</th>\n",
       "      <th>V61</th>\n",
       "      <th>V62</th>\n",
       "      <th>V63</th>\n",
       "      <th>V64</th>\n",
       "      <th>V65</th>\n",
       "      <th>V66</th>\n",
       "      <th>V67</th>\n",
       "      <th>V68</th>\n",
       "      <th>V69</th>\n",
       "      <th>V70</th>\n",
       "      <th>V71</th>\n",
       "      <th>V72</th>\n",
       "      <th>V73</th>\n",
       "      <th>V74</th>\n",
       "      <th>V75</th>\n",
       "      <th>V76</th>\n",
       "      <th>V77</th>\n",
       "      <th>V78</th>\n",
       "      <th>V79</th>\n",
       "      <th>V80</th>\n",
       "      <th>V81</th>\n",
       "      <th>V82</th>\n",
       "      <th>V83</th>\n",
       "      <th>V84</th>\n",
       "      <th>V85</th>\n",
       "      <th>V86</th>\n",
       "      <th>V87</th>\n",
       "      <th>V88</th>\n",
       "      <th>V89</th>\n",
       "      <th>V90</th>\n",
       "      <th>V91</th>\n",
       "      <th>V92</th>\n",
       "      <th>V93</th>\n",
       "      <th>V94</th>\n",
       "      <th>V95</th>\n",
       "      <th>V96</th>\n",
       "      <th>V97</th>\n",
       "      <th>V98</th>\n",
       "      <th>V99</th>\n",
       "      <th>V100</th>\n",
       "      <th>V101</th>\n",
       "      <th>V102</th>\n",
       "      <th>V103</th>\n",
       "      <th>V104</th>\n",
       "      <th>V105</th>\n",
       "      <th>V106</th>\n",
       "      <th>V107</th>\n",
       "      <th>V108</th>\n",
       "      <th>V109</th>\n",
       "      <th>V110</th>\n",
       "      <th>V111</th>\n",
       "      <th>V112</th>\n",
       "      <th>V113</th>\n",
       "      <th>V114</th>\n",
       "      <th>V115</th>\n",
       "      <th>V116</th>\n",
       "      <th>V117</th>\n",
       "      <th>V118</th>\n",
       "      <th>V119</th>\n",
       "      <th>V120</th>\n",
       "      <th>V121</th>\n",
       "      <th>V122</th>\n",
       "      <th>V123</th>\n",
       "      <th>V124</th>\n",
       "      <th>V125</th>\n",
       "      <th>V126</th>\n",
       "      <th>V127</th>\n",
       "      <th>V128</th>\n",
       "      <th>V129</th>\n",
       "      <th>V130</th>\n",
       "      <th>V131</th>\n",
       "      <th>V132</th>\n",
       "      <th>V133</th>\n",
       "      <th>V134</th>\n",
       "      <th>V135</th>\n",
       "      <th>V136</th>\n",
       "      <th>V137</th>\n",
       "      <th>V138</th>\n",
       "      <th>V139</th>\n",
       "      <th>V140</th>\n",
       "      <th>V141</th>\n",
       "      <th>V142</th>\n",
       "      <th>V143</th>\n",
       "      <th>V144</th>\n",
       "      <th>V145</th>\n",
       "      <th>V146</th>\n",
       "      <th>V147</th>\n",
       "      <th>V148</th>\n",
       "      <th>V149</th>\n",
       "      <th>V150</th>\n",
       "      <th>V151</th>\n",
       "      <th>V152</th>\n",
       "      <th>V153</th>\n",
       "      <th>V154</th>\n",
       "      <th>V155</th>\n",
       "      <th>V156</th>\n",
       "      <th>V157</th>\n",
       "      <th>V158</th>\n",
       "      <th>V159</th>\n",
       "      <th>V160</th>\n",
       "      <th>V161</th>\n",
       "      <th>V162</th>\n",
       "      <th>V163</th>\n",
       "      <th>V164</th>\n",
       "      <th>V165</th>\n",
       "      <th>V166</th>\n",
       "      <th>V167</th>\n",
       "      <th>V168</th>\n",
       "      <th>V169</th>\n",
       "      <th>V170</th>\n",
       "      <th>V171</th>\n",
       "      <th>V172</th>\n",
       "      <th>V173</th>\n",
       "      <th>V174</th>\n",
       "      <th>V175</th>\n",
       "      <th>V176</th>\n",
       "      <th>V177</th>\n",
       "      <th>V178</th>\n",
       "      <th>V179</th>\n",
       "      <th>V180</th>\n",
       "      <th>V181</th>\n",
       "      <th>V182</th>\n",
       "      <th>V183</th>\n",
       "      <th>V184</th>\n",
       "      <th>V185</th>\n",
       "      <th>V186</th>\n",
       "      <th>V187</th>\n",
       "      <th>V188</th>\n",
       "      <th>V189</th>\n",
       "      <th>V190</th>\n",
       "      <th>V191</th>\n",
       "      <th>V192</th>\n",
       "      <th>V193</th>\n",
       "      <th>V194</th>\n",
       "      <th>V195</th>\n",
       "      <th>V196</th>\n",
       "      <th>V197</th>\n",
       "      <th>V198</th>\n",
       "      <th>V199</th>\n",
       "      <th>V200</th>\n",
       "      <th>V201</th>\n",
       "      <th>V202</th>\n",
       "      <th>V203</th>\n",
       "      <th>V204</th>\n",
       "      <th>V205</th>\n",
       "      <th>V206</th>\n",
       "      <th>V207</th>\n",
       "      <th>V208</th>\n",
       "      <th>V209</th>\n",
       "      <th>V210</th>\n",
       "      <th>V211</th>\n",
       "      <th>V212</th>\n",
       "      <th>V213</th>\n",
       "      <th>V214</th>\n",
       "      <th>V215</th>\n",
       "      <th>V216</th>\n",
       "      <th>V217</th>\n",
       "      <th>V218</th>\n",
       "      <th>V219</th>\n",
       "      <th>V220</th>\n",
       "      <th>V221</th>\n",
       "      <th>V222</th>\n",
       "      <th>V223</th>\n",
       "      <th>V224</th>\n",
       "      <th>V225</th>\n",
       "      <th>V226</th>\n",
       "      <th>V227</th>\n",
       "      <th>V228</th>\n",
       "      <th>V229</th>\n",
       "      <th>V230</th>\n",
       "      <th>V231</th>\n",
       "      <th>V232</th>\n",
       "      <th>V233</th>\n",
       "      <th>V234</th>\n",
       "      <th>V235</th>\n",
       "      <th>V236</th>\n",
       "      <th>V237</th>\n",
       "      <th>V238</th>\n",
       "      <th>V239</th>\n",
       "      <th>V240</th>\n",
       "      <th>V241</th>\n",
       "      <th>V242</th>\n",
       "      <th>V243</th>\n",
       "      <th>V244</th>\n",
       "      <th>V245</th>\n",
       "      <th>V246</th>\n",
       "      <th>V247</th>\n",
       "      <th>V248</th>\n",
       "      <th>V249</th>\n",
       "      <th>V250</th>\n",
       "      <th>V251</th>\n",
       "      <th>V252</th>\n",
       "      <th>V253</th>\n",
       "      <th>V254</th>\n",
       "      <th>V255</th>\n",
       "      <th>V256</th>\n",
       "      <th>V257</th>\n",
       "      <th>V258</th>\n",
       "      <th>V259</th>\n",
       "      <th>V260</th>\n",
       "      <th>V261</th>\n",
       "      <th>V262</th>\n",
       "      <th>V263</th>\n",
       "      <th>V264</th>\n",
       "      <th>V265</th>\n",
       "      <th>V266</th>\n",
       "      <th>V267</th>\n",
       "      <th>V268</th>\n",
       "      <th>V269</th>\n",
       "      <th>V270</th>\n",
       "      <th>V271</th>\n",
       "      <th>V272</th>\n",
       "      <th>V273</th>\n",
       "      <th>V274</th>\n",
       "      <th>V275</th>\n",
       "      <th>V276</th>\n",
       "      <th>V277</th>\n",
       "      <th>V278</th>\n",
       "      <th>V279</th>\n",
       "      <th>V280</th>\n",
       "      <th>V281</th>\n",
       "      <th>V282</th>\n",
       "      <th>V283</th>\n",
       "      <th>V284</th>\n",
       "      <th>V285</th>\n",
       "      <th>V286</th>\n",
       "      <th>V287</th>\n",
       "      <th>V288</th>\n",
       "      <th>V289</th>\n",
       "      <th>V290</th>\n",
       "      <th>V291</th>\n",
       "      <th>V292</th>\n",
       "      <th>V293</th>\n",
       "      <th>V294</th>\n",
       "      <th>V295</th>\n",
       "      <th>V296</th>\n",
       "      <th>V297</th>\n",
       "      <th>V298</th>\n",
       "      <th>V299</th>\n",
       "      <th>V300</th>\n",
       "      <th>V301</th>\n",
       "      <th>V302</th>\n",
       "      <th>V303</th>\n",
       "      <th>V304</th>\n",
       "      <th>V305</th>\n",
       "      <th>V306</th>\n",
       "      <th>V307</th>\n",
       "      <th>V308</th>\n",
       "      <th>V309</th>\n",
       "      <th>V310</th>\n",
       "      <th>V311</th>\n",
       "      <th>V312</th>\n",
       "      <th>V313</th>\n",
       "      <th>V314</th>\n",
       "      <th>V315</th>\n",
       "      <th>V316</th>\n",
       "      <th>V317</th>\n",
       "      <th>V318</th>\n",
       "      <th>V319</th>\n",
       "      <th>V320</th>\n",
       "      <th>V321</th>\n",
       "      <th>V322</th>\n",
       "      <th>V323</th>\n",
       "      <th>V324</th>\n",
       "      <th>V325</th>\n",
       "      <th>V326</th>\n",
       "      <th>V327</th>\n",
       "      <th>V328</th>\n",
       "      <th>V329</th>\n",
       "      <th>V330</th>\n",
       "      <th>V331</th>\n",
       "      <th>V332</th>\n",
       "      <th>V333</th>\n",
       "      <th>V334</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>M2</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M0</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>330.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>outlook.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>315.0</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>M0</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>476.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M0</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1758.0</td>\n",
       "      <td>925.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1758.0</td>\n",
       "      <td>925.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>420.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1803.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15557.990234</td>\n",
       "      <td>169690.796875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>5155.0</td>\n",
       "      <td>2840.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5   card6  addr1  addr2  dist1  dist2  \\\n",
       "0    NaN  150.0    discover  142.0  credit  315.0   87.0   19.0    NaN   \n",
       "1  404.0  150.0  mastercard  102.0  credit  325.0   87.0    NaN    NaN   \n",
       "2  490.0  150.0        visa  166.0   debit  330.0   87.0  287.0    NaN   \n",
       "3  567.0  150.0  mastercard  117.0   debit  476.0   87.0    NaN    NaN   \n",
       "4  514.0  150.0  mastercard  102.0  credit  420.0   87.0    NaN    NaN   \n",
       "\n",
       "  P_emaildomain R_emaildomain   C1   C2   C3   C4   C5   C6   C7   C8   C9  \\\n",
       "0           NaN           NaN  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0   \n",
       "1     gmail.com           NaN  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "2   outlook.com           NaN  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0   \n",
       "3     yahoo.com           NaN  2.0  5.0  0.0  0.0  0.0  4.0  0.0  0.0  1.0   \n",
       "4     gmail.com           NaN  1.0  1.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0   \n",
       "\n",
       "   C10  C11  C12   C13  C14     D1     D2    D3    D4   D5  D6  D7  D8  D9  \\\n",
       "0  0.0  2.0  0.0   1.0  1.0   14.0    NaN  13.0   NaN  NaN NaN NaN NaN NaN   \n",
       "1  0.0  1.0  0.0   1.0  1.0    0.0    NaN   NaN   0.0  NaN NaN NaN NaN NaN   \n",
       "2  0.0  1.0  0.0   1.0  1.0    0.0    NaN   NaN   0.0  NaN NaN NaN NaN NaN   \n",
       "3  0.0  1.0  0.0  25.0  1.0  112.0  112.0   0.0  94.0  0.0 NaN NaN NaN NaN   \n",
       "4  1.0  1.0  0.0   1.0  1.0    0.0    NaN   NaN   NaN  NaN NaN NaN NaN NaN   \n",
       "\n",
       "    D10    D11  D12  D13  D14    D15   M1   M2   M3   M4   M5   M6   M7   M8  \\\n",
       "0  13.0   13.0  NaN  NaN  NaN    0.0    T    T    T   M2    F    T  NaN  NaN   \n",
       "1   0.0    NaN  NaN  NaN  NaN    0.0  NaN  NaN  NaN   M0    T    T  NaN  NaN   \n",
       "2   0.0  315.0  NaN  NaN  NaN  315.0    T    T    T   M0    F    F    F    F   \n",
       "3  84.0    NaN  NaN  NaN  NaN  111.0  NaN  NaN  NaN   M0    T    F  NaN  NaN   \n",
       "4   NaN    NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "    M9   V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  V11  V12  V13  V14  \\\n",
       "0  NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0   \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  0.0  0.0  1.0   \n",
       "2    F  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0   \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0   \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   V15  V16  V17  V18  V19  V20  V21  V22  V23  V24  V25  V26  V27  V28  V29  \\\n",
       "0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0   \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   V30  V31  V32  V33  V34  V35  V36  V37  V38  V39  V40  V41  V42  V43  V44  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0   \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   V45  V46  V47  V48  V49  V50  V51  V52  V53  V54  V55  V56  V57  V58  V59  \\\n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0  0.0  0.0  0.0   \n",
       "1  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0   \n",
       "2  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0   \n",
       "3  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0   \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   V60  V61  V62  V63  V64  V65  V66  V67  V68  V69  V70  V71  V72  V73  V74  \\\n",
       "0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   V75  V76  V77  V78  V79  V80  V81  V82  V83  V84  V85  V86  V87  V88  V89  \\\n",
       "0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0   \n",
       "1  0.0  0.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0   \n",
       "2  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0   \n",
       "3  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0   \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   V90  V91  V92  V93  V94  V95   V96   V97  V98   V99  V100  V101  V102  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0   1.0   0.0  0.0   0.0   0.0   0.0   1.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  0.0   0.0   0.0   0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  0.0   0.0   0.0   0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  1.0  48.0  28.0  0.0  10.0   4.0   1.0  38.0   \n",
       "4  NaN  NaN  NaN  NaN  NaN  0.0   0.0   0.0  0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V103  V104  V105  V106  V107  V108  V109  V110  V111  V112  V113  V114  \\\n",
       "0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "1   0.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "2   0.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "3  24.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "4   0.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "\n",
       "   V115  V116  V117  V118  V119  V120  V121  V122  V123  V124  V125  V126  \\\n",
       "0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   0.0   \n",
       "1   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   0.0   \n",
       "2   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   0.0   \n",
       "3   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  50.0   \n",
       "4   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   0.0   \n",
       "\n",
       "     V127   V128  V129   V130   V131  V132    V133   V134  V135  V136  V137  \\\n",
       "0   117.0    0.0   0.0    0.0    0.0   0.0   117.0    0.0   0.0   0.0   0.0   \n",
       "1     0.0    0.0   0.0    0.0    0.0   0.0     0.0    0.0   0.0   0.0   0.0   \n",
       "2     0.0    0.0   0.0    0.0    0.0   0.0     0.0    0.0   0.0   0.0   0.0   \n",
       "3  1758.0  925.0   0.0  354.0  135.0  50.0  1404.0  790.0   0.0   0.0   0.0   \n",
       "4     0.0    0.0   0.0    0.0    0.0   0.0     0.0    0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V138  V139  V140  V141  V142  V143  V144   V145  V146  V147  V148  V149  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN    NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN    NaN   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN    NaN   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN    NaN   NaN   NaN   NaN   NaN   \n",
       "4   0.0   0.0   0.0   0.0   0.0   6.0  18.0  140.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "     V150  V151  V152  V153  V154  V155  V156  V157  V158          V159  \\\n",
       "0     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN           NaN   \n",
       "1     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN           NaN   \n",
       "2     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN           NaN   \n",
       "3     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN           NaN   \n",
       "4  1803.0  49.0  64.0   0.0   0.0   0.0   0.0   0.0   0.0  15557.990234   \n",
       "\n",
       "            V160  V161  V162  V163   V164    V165    V166  V167  V168  V169  \\\n",
       "0            NaN   NaN   NaN   NaN    NaN     NaN     NaN   NaN   NaN   NaN   \n",
       "1            NaN   NaN   NaN   NaN    NaN     NaN     NaN   NaN   NaN   NaN   \n",
       "2            NaN   NaN   NaN   NaN    NaN     NaN     NaN   NaN   NaN   NaN   \n",
       "3            NaN   NaN   NaN   NaN    NaN     NaN     NaN   NaN   NaN   NaN   \n",
       "4  169690.796875   0.0   0.0   0.0  515.0  5155.0  2840.0   0.0   0.0   0.0   \n",
       "\n",
       "   V170  V171  V172  V173  V174  V175  V176  V177  V178  V179  V180  V181  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4   1.0   1.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V182  V183  V184  V185  V186  V187  V188  V189  V190  V191  V192  V193  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4   0.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "\n",
       "   V194  V195  V196  V197  V198  V199  V200  V201  V202  V203  V204  V205  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V206  V207  V208  V209  V210  V211  V212  V213  V214  V215  V216  V217  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V218  V219  V220  V221  V222  V223  V224  V225  V226  V227  V228  V229  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   \n",
       "\n",
       "   V230  V231  V232  V233  V234  V235  V236  V237  V238  V239  V240  V241  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   \n",
       "\n",
       "   V242  V243  V244  V245  V246  V247  V248  V249  V250  V251  V252  V253  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "\n",
       "   V254  V255  V256  V257  V258  V259  V260  V261  V262  V263  V264  V265  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   0.0   0.0   0.0   \n",
       "\n",
       "   V266  V267  V268  V269  V270  V271  V272  V273  V274  V275  V276  V277  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V278  V279  V280  V281  V282  V283  V284  V285  V286  V287  V288  V289  \\\n",
       "0   NaN   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1   NaN   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2   NaN   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3   NaN   1.0  28.0   0.0   0.0   0.0   0.0  10.0   0.0   4.0   0.0   0.0   \n",
       "4   0.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V290  V291  V292  V293  V294  V295  V296  V297  V298  V299  V300  V301  \\\n",
       "0   1.0   1.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1   1.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2   1.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3   1.0   1.0   1.0   1.0  38.0  24.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4   1.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V302  V303  V304  V305  V306    V307   V308  V309   V310  V311   V312  \\\n",
       "0   0.0   0.0   0.0   1.0   0.0   117.0    0.0   0.0    0.0   0.0    0.0   \n",
       "1   0.0   0.0   0.0   1.0   0.0     0.0    0.0   0.0    0.0   0.0    0.0   \n",
       "2   0.0   0.0   0.0   1.0   0.0     0.0    0.0   0.0    0.0   0.0    0.0   \n",
       "3   0.0   0.0   0.0   1.0  50.0  1758.0  925.0   0.0  354.0   0.0  135.0   \n",
       "4   1.0   1.0   1.0   1.0   0.0     0.0    0.0   0.0    0.0   0.0    0.0   \n",
       "\n",
       "   V313  V314  V315  V316    V317   V318  V319  V320  V321  V322  V323  V324  \\\n",
       "0   0.0   0.0   0.0   0.0   117.0    0.0   0.0   0.0   0.0   NaN   NaN   NaN   \n",
       "1   0.0   0.0   0.0   0.0     0.0    0.0   0.0   0.0   0.0   NaN   NaN   NaN   \n",
       "2   0.0   0.0   0.0   0.0     0.0    0.0   0.0   0.0   0.0   NaN   NaN   NaN   \n",
       "3   0.0   0.0   0.0  50.0  1404.0  790.0   0.0   0.0   0.0   NaN   NaN   NaN   \n",
       "4   0.0   0.0   0.0   0.0     0.0    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V325  V326  V327  V328  V329  V330  V331  V332  V333  V334  V335  V336  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V337  V338  V339  \n",
       "0   NaN   NaN   NaN  \n",
       "1   NaN   NaN   NaN  \n",
       "2   NaN   NaN   NaN  \n",
       "3   NaN   NaN   NaN  \n",
       "4   0.0   0.0   0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08439a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>id_01</th>\n",
       "      <th>id_02</th>\n",
       "      <th>id_03</th>\n",
       "      <th>id_04</th>\n",
       "      <th>id_05</th>\n",
       "      <th>id_06</th>\n",
       "      <th>id_07</th>\n",
       "      <th>id_08</th>\n",
       "      <th>id_09</th>\n",
       "      <th>id_10</th>\n",
       "      <th>id_11</th>\n",
       "      <th>id_12</th>\n",
       "      <th>id_13</th>\n",
       "      <th>id_14</th>\n",
       "      <th>id_15</th>\n",
       "      <th>id_16</th>\n",
       "      <th>id_17</th>\n",
       "      <th>id_18</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "      <th>id_21</th>\n",
       "      <th>id_22</th>\n",
       "      <th>id_23</th>\n",
       "      <th>id_24</th>\n",
       "      <th>id_25</th>\n",
       "      <th>id_26</th>\n",
       "      <th>id_27</th>\n",
       "      <th>id_28</th>\n",
       "      <th>id_29</th>\n",
       "      <th>id_30</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70787.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-480.0</td>\n",
       "      <td>New</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>166.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>542.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>Android 7.0</td>\n",
       "      <td>samsung browser 6.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2220x1080</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987008</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>98945.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>49.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>New</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>166.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>621.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>iOS 11.1.2</td>\n",
       "      <td>mobile safari 11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1334x750</td>\n",
       "      <td>match_status:1</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>iOS Device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987010</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>191631.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Found</td>\n",
       "      <td>Found</td>\n",
       "      <td>121.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>410.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Found</td>\n",
       "      <td>Found</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chrome 62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987011</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>221832.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chrome 62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>desktop</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7460.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>Found</td>\n",
       "      <td>Found</td>\n",
       "      <td>166.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>529.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Found</td>\n",
       "      <td>Found</td>\n",
       "      <td>Mac OS X 10_11_6</td>\n",
       "      <td>chrome 62.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1280x800</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>desktop</td>\n",
       "      <td>MacOS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  id_01     id_02  id_03  id_04  id_05  id_06  id_07  id_08  \\\n",
       "0        2987004    0.0   70787.0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1        2987008   -5.0   98945.0    NaN    NaN    0.0   -5.0    NaN    NaN   \n",
       "2        2987010   -5.0  191631.0    0.0    0.0    0.0    0.0    NaN    NaN   \n",
       "3        2987011   -5.0  221832.0    NaN    NaN    0.0   -6.0    NaN    NaN   \n",
       "4        2987016    0.0    7460.0    0.0    0.0    1.0    0.0    NaN    NaN   \n",
       "\n",
       "   id_09  id_10  id_11     id_12  id_13  id_14  id_15     id_16  id_17  id_18  \\\n",
       "0    NaN    NaN  100.0  NotFound    NaN -480.0    New  NotFound  166.0    NaN   \n",
       "1    NaN    NaN  100.0  NotFound   49.0 -300.0    New  NotFound  166.0    NaN   \n",
       "2    0.0    0.0  100.0  NotFound   52.0    NaN  Found     Found  121.0    NaN   \n",
       "3    NaN    NaN  100.0  NotFound   52.0    NaN    New  NotFound  225.0    NaN   \n",
       "4    0.0    0.0  100.0  NotFound    NaN -300.0  Found     Found  166.0   15.0   \n",
       "\n",
       "   id_19  id_20  id_21  id_22 id_23  id_24  id_25  id_26 id_27  id_28  \\\n",
       "0  542.0  144.0    NaN    NaN   NaN    NaN    NaN    NaN   NaN    New   \n",
       "1  621.0  500.0    NaN    NaN   NaN    NaN    NaN    NaN   NaN    New   \n",
       "2  410.0  142.0    NaN    NaN   NaN    NaN    NaN    NaN   NaN  Found   \n",
       "3  176.0  507.0    NaN    NaN   NaN    NaN    NaN    NaN   NaN    New   \n",
       "4  529.0  575.0    NaN    NaN   NaN    NaN    NaN    NaN   NaN  Found   \n",
       "\n",
       "      id_29             id_30                id_31  id_32      id_33  \\\n",
       "0  NotFound       Android 7.0  samsung browser 6.2   32.0  2220x1080   \n",
       "1  NotFound        iOS 11.1.2   mobile safari 11.0   32.0   1334x750   \n",
       "2     Found               NaN          chrome 62.0    NaN        NaN   \n",
       "3  NotFound               NaN          chrome 62.0    NaN        NaN   \n",
       "4     Found  Mac OS X 10_11_6          chrome 62.0   24.0   1280x800   \n",
       "\n",
       "            id_34 id_35 id_36 id_37 id_38 DeviceType  \\\n",
       "0  match_status:2     T     F     T     T     mobile   \n",
       "1  match_status:1     T     F     F     T     mobile   \n",
       "2             NaN     F     F     T     T    desktop   \n",
       "3             NaN     F     F     T     T    desktop   \n",
       "4  match_status:2     T     F     T     T    desktop   \n",
       "\n",
       "                      DeviceInfo  \n",
       "0  SAMSUNG SM-G892A Build/NRD90M  \n",
       "1                     iOS Device  \n",
       "2                        Windows  \n",
       "3                            NaN  \n",
       "4                          MacOS  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f356bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (5, 5))\n",
    "\n",
    "sns.barplot(x = [0,1], y = train['isFraud'].value_counts().values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "124991fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03499000914417313\n"
     ]
    }
   ],
   "source": [
    "fraud_ratio = train['isFraud'].sum()/len(train['isFraud'])\n",
    "print(fraud_ratio) # percentage of frauds in the train set\n",
    "del fraud_ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cfc53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(columns = ['train_time', 'precision', 'accuracy', 'recall', 'roc_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ff67e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Unbalanced DT - train precision score is 0.8881843489019486\n",
      "Unbalanced DT - train accuracy score is 0.975490652623023\n",
      "Unbalanced DT - train recall score is 0.3459846978733659\n",
      "Unbalanced DT - train auc score is 0.6721992953403\n",
      "-----------------------------------------------------\n",
      "Unbalanced DT - test precision score is 0.6646115906288532\n",
      "Unbalanced DT - test accuracy score is 0.9701121007891083\n",
      "Unbalanced DT - test recall score is 0.265255905511811\n",
      "Unbalanced DT - test auc score is 0.6302429083870655\n",
      "-----------------------------------------------------\n",
      " Train confusion matrix\n",
      "[[455110    723]\n",
      " [ 10856   5743]]\n",
      "-----------------------------------------------------\n",
      " Test confusion matrix\n",
      "[[113500    544]\n",
      " [  2986   1078]]\n",
      "-----------------------------------------------------\n",
      "Wall time: 5h 12min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# UNBALANCED APPROACH - DECISION TREE\n",
    "\n",
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "X = train.copy()\n",
    "y = train['isFraud'].copy()\n",
    "X = X.drop(['isFraud'], axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17, shuffle=False)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create our imputer to replace missing values with the mean e.g.\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp = imp.fit(X_train)\n",
    "\n",
    "# Impute our data, then train\n",
    "X_train_imp = imp.transform(X_train)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "## ------------------------------------------- ##\n",
    "## RANDOM SEARCH TUNING - IT ONLY RUNS ONCE\n",
    "#from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#clf_model = DecisionTreeClassifier(criterion=\"gini\")\n",
    "#distrib = dict(max_depth = [10,100,500], min_samples_leaf=[5,10,20,50])\n",
    "#clf = RandomizedSearchCV(clf_model, distrib, random_state=17)\n",
    "#search = clf.fit(X_train_imp,y_train)\n",
    "#clf_model = DecisionTreeClassifier(criterion=\"gini\", max_depth = search.best_params_['max_depth'], min_samples_leaf = search.best_params_['min_samples_leaf'])\n",
    "\n",
    "# optimal values are: 'min_samples_leaf': 20, 'max_depth': 10\n",
    "## ------------------------------------------- ##\n",
    "\n",
    "clf_model = DecisionTreeClassifier(criterion=\"gini\", max_depth = 10, min_samples_leaf = 20)\n",
    "clf_model.fit(X_train_imp,y_train)\n",
    "y_pred = []\n",
    "X_test_imp = imp.transform(X_test)\n",
    "y_pred = clf_model.predict(X_test_imp)\n",
    "y_train_pred = clf_model.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Unbalanced DT - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Unbalanced DT - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Unbalanced DT - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Unbalanced DT - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Unbalanced DT - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Unbalanced DT - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Unbalanced DT - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Unbalanced DT - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "end_time = datetime.datetime.now() - start_time\n",
    "\n",
    "stats_df.loc['UNBALANCED APPROACH - DECISION TREE'] = ([end_time,\n",
    "                                                         precision_score(y_train, y_train_pred),\n",
    "                                                         accuracy_score(y_train, y_train_pred),\n",
    "                                                         recall_score(y_train, y_train_pred),\n",
    "                                                         roc_auc_score(y_train, y_train_pred)\n",
    "                                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2167488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 20, 'max_depth': 10}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a2a9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:57:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:57:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:05:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:05:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:13:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:13:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:21:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:21:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:29:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:29:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:37:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:37:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:37:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:37:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:38:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:38:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:38:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:38:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:39:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:39:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:39:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:39:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:41:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:41:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:43:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:43:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:45:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:45:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:47:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:49:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:49:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:52:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:52:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:56:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:59:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:59:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:03:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:03:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:07:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:07:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:24:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:24:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:41:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:41:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:57:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:57:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:14:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:14:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:32:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:33:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:36:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:37:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:40:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:40:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:44:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:44:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:48:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:48:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:51:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:52:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:06:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:07:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:21:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:21:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:35:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:35:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:49:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:49:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:02:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:02:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:04:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:05:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:06:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:07:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:07:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:09:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:09:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:11:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:11:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:12:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:12:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:14:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:14:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:16:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:16:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:17:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:18:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:19:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:19:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:23:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:28:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:28:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:32:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:32:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:36:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:37:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:41:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[16:59:59] C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_gpu_hist.cu:793: Exception in gpu_hist: [16:59:59] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.4.0\\src\\tree\\param.h:244: Check failed: this->max_depth <= 31 (100 vs. 31) : max_depth can not be greater than 31 as that might generate 2 ** 32 - 1 nodes.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1174\u001b[0m         )\n\u001b[0;32m   1175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         self._Booster = train(\n\u001b[0m\u001b[0;32m   1177\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m--> 189\u001b[1;33m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    190\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1499\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1500\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1501\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [16:59:59] C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_gpu_hist.cu:793: Exception in gpu_hist: [16:59:59] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.4.0\\src\\tree\\param.h:244: Check failed: this->max_depth <= 31 (100 vs. 31) : max_depth can not be greater than 31 as that might generate 2 ** 32 - 1 nodes.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# UNBALANCED APPROACH - XGBOOST\n",
    "\n",
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "## ------------------------------------------- ##\n",
    "## RANDOM SEARCH TUNING - IT ONLY RUNS ONCE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "clf_model = xgb.XGBClassifier(criterion=\"gini\")\n",
    "distrib = dict(max_depth = [1,5,10], n_estimators = [10, 50, 100], learning_rate=[0.02, 0.1, 0.5])\n",
    "clf = RandomizedSearchCV(clf_model, distrib, random_state=17)\n",
    "search = clf.fit(X_train_imp,y_train)\n",
    "xgmodel = xgb.XGBClassifier(tree_method = 'gpu_hist',\n",
    "                           max_depth = search.best_params_['max_depth'],\n",
    "                           n_estimators = search.best_params_['n_estimators'],\n",
    "                           learning_rate = search.best_params_['learning_rate'])\n",
    "\n",
    "# optimal values are: 'n_estimators': , 'max_depth': . learning_rate: \n",
    "## ------------------------------------------- ##\n",
    "\n",
    "#xgmodel = xgb.XGBClassifier(tree_method = 'gpu_hist')\n",
    "\n",
    "xgmodel.fit(X_train_imp,y_train)\n",
    "\n",
    "y_pred = []\n",
    "X_test_imp = imp.transform(X_test)\n",
    "y_pred = xgmodel.predict(X_test_imp)\n",
    "y_train_pred = xgmodel.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Unbalanced XGB - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Unbalanced XGB - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Unbalanced XGB - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Unbalanced XGB - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Unbalanced XGB - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Unbalanced XGB - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Unbalanced XGB - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Unbalanced XGB - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now() - start_time\n",
    "\n",
    "stats_df.loc['UNBALANCED APPROACH - XGBOOST'] = ([end_time,\n",
    "                                                         precision_score(y_train, y_train_pred),\n",
    "                                                         accuracy_score(y_train, y_train_pred),\n",
    "                                                         recall_score(y_train, y_train_pred),\n",
    "                                                         roc_auc_score(y_train, y_train_pred)\n",
    "                                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3cdeb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 16604, 1: 16604})\n",
      "-----------------------------------------------------\n",
      "Undersampled DT - train precision score is 0.9336237040672352\n",
      "Undersampled DT - train accuracy score is 0.9257106721271983\n",
      "Undersampled DT - train recall score is 0.91658636473139\n",
      "Undersampled DT - train auc score is 0.9257106721271984\n",
      "-----------------------------------------------------\n",
      "Undersampled DT - test precision score is 0.12459387986399698\n",
      "Undersampled DT - test accuracy score is 0.7973634300809429\n",
      "Undersampled DT - test recall score is 0.8125153978812515\n",
      "Undersampled DT - test auc score is 0.8046697849738219\n",
      "-----------------------------------------------------\n",
      " Train confusion matrix\n",
      "[[15522  1082]\n",
      " [ 1385 15219]]\n",
      "-----------------------------------------------------\n",
      " Test confusion matrix\n",
      "[[90877 23172]\n",
      " [  761  3298]]\n",
      "-----------------------------------------------------\n",
      "Wall time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## UNDERSAMPLING APPROACH - DECISION TREE\n",
    "\n",
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "X = train.copy()\n",
    "y = train['isFraud'].copy()\n",
    "X = X.drop(['isFraud'], axis=1)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create our imputer to replace missing values with the mean e.g.\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp = imp.fit(X)\n",
    "\n",
    "# Impute our data, then train\n",
    "X_imp = imp.transform(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.2, random_state=17, shuffle=False)\n",
    "\n",
    "ros = RandomUnderSampler(random_state=17)\n",
    "\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_train)))\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_model = DecisionTreeClassifier(criterion=\"gini\", random_state=17,max_depth=100, min_samples_leaf=5)   \n",
    "clf_model.fit(X_train,y_train)\n",
    "y_pred = []\n",
    "y_pred = clf_model.predict(X_test)\n",
    "y_train_pred = clf_model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Undersampled DT - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Undersampled DT - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Undersampled DT - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Undersampled DT - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Undersampled DT - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Undersampled DT - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Undersampled DT - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Undersampled DT - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now() - start_time\n",
    "\n",
    "stats_df.loc['UNDERSAMPLING APPROACH - DECISION TREE'] = ([end_time,\n",
    "                                                         precision_score(y_train, y_train_pred),\n",
    "                                                         accuracy_score(y_train, y_train_pred),\n",
    "                                                         recall_score(y_train, y_train_pred),\n",
    "                                                         roc_auc_score(y_train, y_train_pred)\n",
    "                                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6f49d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-----------------------------------------------------\n",
      "Undersampled XGB - train precision score is 0.9445770352691594\n",
      "Undersampled XGB - train accuracy score is 0.9038785834738617\n",
      "Undersampled XGB - train recall score is 0.8581064803661769\n",
      "Undersampled XGB - train auc score is 0.9038785834738617\n",
      "-----------------------------------------------------\n",
      "Undersampled XGB - test precision score is 0.20513931888544892\n",
      "Undersampled XGB - test accuracy score is 0.8849950892403563\n",
      "Undersampled XGB - test recall score is 0.8162108893816211\n",
      "Undersampled XGB - test auc score is 0.851827002968393\n",
      "-----------------------------------------------------\n",
      " Train confusion matrix\n",
      "[[15768   836]\n",
      " [ 2356 14248]]\n",
      "-----------------------------------------------------\n",
      " Test confusion matrix\n",
      "[[101212  12837]\n",
      " [   746   3313]]\n",
      "-----------------------------------------------------\n",
      "Wall time: 49.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## UNDERSAMPLING APPROACH - XGBOOST\n",
    "\n",
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "xgmodel = xgb.XGBClassifier(n_estimators = 100,\n",
    "                            max_depth = 12,\n",
    "                            learning_rate = 0.02,\n",
    "                            subsample = 0.8,\n",
    "                            colsample_bytree = 0.4,\n",
    "                            missing = -1,\n",
    "                            random_state = 42,\n",
    "                            tree_method = 'gpu_hist')\n",
    "xgmodel.fit(X_train,y_train)\n",
    "\n",
    "y_pred = []\n",
    "y_pred = xgmodel.predict(X_test)\n",
    "y_train_pred = xgmodel.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Undersampled XGB - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Undersampled XGB - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Undersampled XGB - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Undersampled XGB - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Undersampled XGB - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Undersampled XGB - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Undersampled XGB - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Undersampled XGB - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now() - start_time\n",
    "\n",
    "stats_df.loc['UNDERSAMPLING APPROACH - XGBOOST'] = ([end_time,\n",
    "                                                         precision_score(y_train, y_train_pred),\n",
    "                                                         accuracy_score(y_train, y_train_pred),\n",
    "                                                         recall_score(y_train, y_train_pred),\n",
    "                                                         roc_auc_score(y_train, y_train_pred)\n",
    "                                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## OVERSAMPLING APPROACH - DECISION TREE\n",
    "\n",
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "ros = RandomOverSampler(random_state=17)\n",
    "X = train.copy()\n",
    "y = train['isFraud'].copy()\n",
    "X = X.drop(['isFraud'], axis=1)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_resampled)))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=17)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create our imputer to replace missing values with the mean e.g.\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp = imp.fit(X_train)\n",
    "\n",
    "# Impute our data, then train\n",
    "X_train_imp = imp.transform(X_train)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_model = DecisionTreeClassifier(criterion=\"gini\", random_state=17,max_depth=100, min_samples_leaf=50)   \n",
    "clf_model.fit(X_train_imp,y_train)\n",
    "y_pred = []\n",
    "X_test_imp = imp.transform(X_test)\n",
    "y_pred = clf_model.predict(X_test_imp)\n",
    "y_train_pred = clf_model.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled DT - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Oversampled DT - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Oversampled DT - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Oversampled DT - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled DT - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Oversampled DT - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Oversampled DT - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Oversampled DT - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')\n",
    "## OVERSAMPLING IS BETTER THAN UNDERSAMPLING.\n",
    "\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now() - start_time\n",
    "\n",
    "stats_df.loc['OVERSAMPLING APPROACH - DECISION TREE'] = ([end_time,\n",
    "                                                         precision_score(y_train, y_train_pred),\n",
    "                                                         accuracy_score(y_train, y_train_pred),\n",
    "                                                         recall_score(y_train, y_train_pred),\n",
    "                                                         roc_auc_score(y_train, y_train_pred)\n",
    "                                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## OVERSAMPLING APPROACH - XGBOOST\n",
    "\n",
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "xgmodel = xgb.XGBClassifier(n_estimators = 100,\n",
    "                            max_depth = 12,\n",
    "                            learning_rate = 0.02,\n",
    "                            subsample = 0.8,\n",
    "                            colsample_bytree = 0.4,\n",
    "                            missing = -1,\n",
    "                            random_state = 42,\n",
    "                            tree_method = 'gpu_hist')\n",
    "xgmodel.fit(X_train_imp,y_train)\n",
    "\n",
    "y_pred = []\n",
    "X_test_imp = imp.transform(X_test)\n",
    "y_pred = xgmodel.predict(X_test_imp)\n",
    "y_train_pred = xgmodel.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled XGB - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled XGB - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now() - start_time\n",
    "\n",
    "stats_df.loc['OVERSAMPLING APPROACH - XGBOOST'] = ([end_time,\n",
    "                                                         precision_score(y_train, y_train_pred),\n",
    "                                                         accuracy_score(y_train, y_train_pred),\n",
    "                                                         recall_score(y_train, y_train_pred),\n",
    "                                                         roc_auc_score(y_train, y_train_pred)\n",
    "                                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET'S PRINT SOME STATS ABOUT THE EXPERIMENTS \n",
    "\n",
    "print(stats_df)\n",
    "\n",
    "## THE BEST OVERALL APPROACH IS DECISION TREE WITH OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c51280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "#from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#clf_model = DecisionTreeClassifier(criterion=\"gini\")\n",
    "#distrib = dict(max_depth = [10,100,500], min_samples_leaf=[5,10,20,50])\n",
    "#clf = RandomizedSearchCV(clf_model, distrib, random_state=17)\n",
    "#search = clf.fit(X_train_imp,y_train)\n",
    "#search.best_params_\n",
    "\n",
    "\n",
    "# search of best params output - {'min_samples_leaf': 5, 'max_depth': 500}\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#clf_model = RandomForestClassifier(max_depth=100, min_samples_leaf=50, n_estimators = 500)   \n",
    "#clf_model.fit(X_train_imp,y_train)\n",
    "#y_pred = []\n",
    "#X_test_imp = imp.transform(X_test)\n",
    "#y_pred = clf_model.predict(X_test_imp)\n",
    "\n",
    "\n",
    "#from sklearn.metrics import precision_score\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "#from sklearn.metrics import recall_score\n",
    "\n",
    "#print('precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "#print('accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "#print('recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "#print('auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "\n",
    "# RANDOMFOREST RESULTS, 500 trees, max depth 100, min leaves 50\n",
    "# precision score is 0.9363672902660041\n",
    "# accuracy score is 0.9202065356151102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69322d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tree_best_clf = DecisionTreeClassifier(criterion=\"gini\", \n",
    "#                                       max_depth = search.best_params_['max_depth'], \n",
    "#                                       min_samples_leaf = search.best_params_['min_samples_leaf'])\n",
    "\n",
    "tree_best_clf = DecisionTreeClassifier(criterion=\"gini\", \n",
    "                                       max_depth = 500, \n",
    "                                       min_samples_leaf = 5)\n",
    "\n",
    "tree_best_clf.fit(X_train_imp,y_train)\n",
    "X_test_imp = imp.transform(X_test)\n",
    "y_pred = tree_best_clf.predict(X_test_imp)\n",
    "y_train_pred = tree_best_clf.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled BESTDT - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Oversampled BESTDT - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Oversampled BESTDT - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Oversampled BESTDT - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled BESTDT - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Oversampled BESTDT - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Oversampled BESTDT - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Oversampled BESTDT - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## OVERSAMPLING APPROACH - XGBOOST 2000 ESTIMATORS (it takes approximately 80 minutes on my laptop)\n",
    "\n",
    "xgmodel = xgb.XGBClassifier(n_estimators = 2000,\n",
    "                            max_depth = 12,\n",
    "                            learning_rate = 0.02,\n",
    "                            subsample = 0.8,\n",
    "                            colsample_bytree = 0.4,\n",
    "                            missing = -1,\n",
    "                            random_state = 42,\n",
    "                            tree_method = 'gpu_hist')\n",
    "xgmodel.fit(X_train_imp,y_train)\n",
    "\n",
    "y_pred = []\n",
    "X_test_imp = imp.transform(X_test)\n",
    "y_pred = xgmodel.predict(X_test_imp)\n",
    "y_train_pred = xgmodel.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled XGB - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled XGB - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## EXP1 - OVERSAMPLING APPROACH ONLY FOR TRAIN (TEST DATA NOT OVERSAMPLED) - DT\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "X = train.copy()\n",
    "y = train['isFraud'].copy()\n",
    "X = X.drop(['isFraud'], axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=17)\n",
    "\n",
    "ros = RandomOverSampler(random_state=17)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_train)))\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create our imputer to replace missing values with the mean e.g.\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp = imp.fit(X_train)\n",
    "\n",
    "# Impute our data, then train\n",
    "X_train_imp = imp.transform(X_train)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_model = DecisionTreeClassifier(criterion=\"gini\", random_state=17,max_depth=100, min_samples_leaf=50)   \n",
    "clf_model.fit(X_train_imp,y_train)\n",
    "y_pred = []\n",
    "X_test_imp = imp.transform(X_test)\n",
    "y_pred = clf_model.predict(X_test_imp)\n",
    "y_train_pred = clf_model.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Train Oversampled DT - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Train Oversampled DT - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Train Oversampled DT - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Train Oversampled DT - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Train Oversampled DT - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Train Oversampled DT - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Train Oversampled DT - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Train Oversampled DT - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## EXP1 - OVERSAMPLING APPROACH ONLY FOR TRAIN (TEST DATA NOT OVERSAMPLED) - XGB\n",
    "\n",
    "xgmodel = xgb.XGBClassifier(n_estimators = 100,\n",
    "                            max_depth = 12,\n",
    "                            learning_rate = 0.02,\n",
    "                            subsample = 0.8,\n",
    "                            colsample_bytree = 0.4,\n",
    "                            missing = -1,\n",
    "                            random_state = 42,\n",
    "                            tree_method = 'gpu_hist')\n",
    "xgmodel.fit(X_train_imp,y_train)\n",
    "\n",
    "y_pred = []\n",
    "X_test_imp = imp.transform(X_test)\n",
    "y_pred = xgmodel.predict(X_test_imp)\n",
    "y_train_pred = xgmodel.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled XGB - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled XGB - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## EXP2 - OVERSAMPLING APPROACH ONLY FOR TRAIN (TEST DATA NOT OVERSAMPLED) - DT\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "X = train.copy()\n",
    "y = train['isFraud'].copy()\n",
    "X = X.drop(['isFraud'], axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=17)\n",
    "\n",
    "ros = RandomOverSampler(random_state=17)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_train)))\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create our imputer to replace missing values with the mean e.g.\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp = imp.fit(X_train)\n",
    "\n",
    "# Impute our data, then train\n",
    "X_train_imp = imp.transform(X_train)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_model = DecisionTreeClassifier(criterion=\"gini\", random_state=17,max_depth=500, min_samples_leaf=5)   \n",
    "clf_model.fit(X_train_imp,y_train)\n",
    "y_pred = []\n",
    "X_test_imp = imp.transform(X_test)\n",
    "y_pred = clf_model.predict(X_test_imp)\n",
    "y_train_pred = clf_model.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Train Oversampled DT - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Train Oversampled DT - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Train Oversampled DT - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Train Oversampled DT - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Train Oversampled DT - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Train Oversampled DT - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Train Oversampled DT - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Train Oversampled DT - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## EXP2 - OVERSAMPLING APPROACH ONLY FOR TRAIN (TEST DATA NOT OVERSAMPLED) - XGB\n",
    "\n",
    "xgmodel = xgb.XGBClassifier(n_estimators = 2000,\n",
    "                            max_depth = 12,\n",
    "                            learning_rate = 0.02,\n",
    "                            subsample = 0.8,\n",
    "                            colsample_bytree = 0.4,\n",
    "                            missing = -1,\n",
    "                            random_state = 42,\n",
    "                            tree_method = 'gpu_hist')\n",
    "xgmodel.fit(X_train_imp,y_train)\n",
    "\n",
    "y_pred = []\n",
    "X_test_imp = imp.transform(X_test)\n",
    "y_pred = xgmodel.predict(X_test_imp)\n",
    "y_train_pred = xgmodel.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled XGB - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled XGB - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74901f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## EXP3 - OVERSAMPLING APPROACH ONLY FOR TRAIN (TEST DATA NOT OVERSAMPLED) - XGB\n",
    "\n",
    "xgmodel = xgb.XGBClassifier(n_estimators = 5000,\n",
    "                            max_depth = 12,\n",
    "                            learning_rate = 0.02,\n",
    "                            subsample = 0.8,\n",
    "                            colsample_bytree = 0.4,\n",
    "                            missing = -1,\n",
    "                            random_state = 42,\n",
    "                            tree_method = 'gpu_hist')\n",
    "xgmodel.fit(X_train_imp,y_train)\n",
    "\n",
    "y_pred = []\n",
    "X_test_imp = imp.transform(X_test)\n",
    "y_pred = xgmodel.predict(X_test_imp)\n",
    "y_train_pred = xgmodel.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled XGB - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Oversampled XGB - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Oversampled XGB - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Oversampled XGB - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "# IT SEEMS THAT THE OVERSAMPLING APPLIED TO BOTH THE TRAINING AND TESTING SET LETS THE ALGORITHM LEARN TOO WELL, \n",
    "#HENCE THE RESULTS OF THE TEST SET ARE BIASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## UNDERSAMPLING APPROACH - XGB\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "ros = RandomUnderSampler(random_state=17)\n",
    "X = train.copy()\n",
    "y = train['isFraud'].copy()\n",
    "X = X.drop(['isFraud'], axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=17)\n",
    "\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_train)))\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create our imputer to replace missing values with the mean e.g.\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp = imp.fit(X_train)\n",
    "\n",
    "# Impute our data, then train\n",
    "X_train_imp = imp.transform(X_train)\n",
    "\n",
    "\n",
    "## UNDERSAMPLING APPROACH - XGBOOST WITH RANDOMSEARCH\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "xgmodel = xgb.XGBClassifier(tree_method = 'gpu_hist', random_state = 17)\n",
    "distrib = dict(n_estimators = [10,100,1000,2000], max_depth = [10,50], learning_rate=[0.02, 0.2, 2])\n",
    "clf = RandomizedSearchCV(xgmodel, distrib, random_state=17)\n",
    "search = clf.fit(X_train_imp,y_train)\n",
    "search.best_params_\n",
    "best_xgb = xgb.XGBClassifier(tree_method = 'gpu_hist', random_state = 17, \n",
    "                             max_depth = search.best_params_['max_depth'],\n",
    "                             n_estimators = search.best_params_['n_estimators'],\n",
    "                             learning_rate = search.best_params_['learning_rate'])\n",
    "\n",
    "\n",
    "\n",
    "y_pred = []\n",
    "X_test_imp = imp.transform(X_test)\n",
    "best_xgb.fit(X_train_imp,y_train)\n",
    "y_pred = best_xgb.predict(X_test_imp)\n",
    "y_train_pred = best_xgb.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Undersampled XGB RandomSearch - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Undersampled XGB RandomSearch - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Undersampled XGB RandomSearch - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Undersampled XGB RandomSearch - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Undersampled XGB RandomSearch - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Undersampled XGB RandomSearch - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Undersampled XGB RandomSearch - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Undersampled XGB RandomSearch - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4990dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86edbf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_pred = best_xgb.predict(X_train_imp)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "print('Undersampled XGB RandomSearch - train precision score is {}'.format(precision_score(y_train, y_train_pred)))\n",
    "print('Undersampled XGB RandomSearch - train accuracy score is {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Undersampled XGB RandomSearch - train recall score is {}'.format(recall_score(y_train, y_train_pred)))\n",
    "print('Undersampled XGB RandomSearch - train auc score is {}'.format(roc_auc_score(y_train, y_train_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print('Undersampled XGB RandomSearch - test precision score is {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Undersampled XGB RandomSearch - test accuracy score is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Undersampled XGB RandomSearch - test recall score is {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Undersampled XGB RandomSearch - test auc score is {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Train confusion matrix')\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print('-----------------------------------------------------')\n",
    "print(' Test confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('-----------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
